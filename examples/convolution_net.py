"""
This example shows how to build a fully-functional text classification application with command-line support for
training and testing with a minimum of code.

The model used in the application is copied from imdb_cnn_lstm.py in the Keras examples folder.

Run "python convolution_net.py --help" to see the command line options. Notice that they identical to what is provided
by the "mycroft" command line application, except the model parameters correspond to the model being trained below.
"""

import mycroft.console
import mycroft.model
import mycroft.text


# Write your own model by extending mycroft.model.TextEmbeddingClassifier. All you need to do is override the
# constructor.
class ConvolutionNetClassifier(mycroft.model.TextEmbeddingClassifier):
    # (It is not necessary to write default parameter values at the top of the class, but I do it here for clarity's
    # sake.)
    VOCABULARY_SIZE = 20000
    DROPOUT = 0.25
    FILTERS = 64
    KERNEL_SIZE = 5
    POOL_SIZE = 4
    LSTM_OUTPUT_SIZE = 70
    LEARNING_RATE = 0.001
    LANGUAGE_MODEL = "en"

    # Mycroft creates command line options for every keyword argument in the constructor. Argument names, types, and
    # defaults are taken from these arguments. Additionally, the constructor must take training as its first
    # positional argument. This will be tuple of (training text, training label, label names). The label names are
    # required by the base class, and the other training data is available in case the model hyper-parameters depend on
    # it.
    def __init__(self, training,
                 sequence_length=None, vocabulary_size=VOCABULARY_SIZE, dropout=DROPOUT, filters=FILTERS,
                 kernel_size=KERNEL_SIZE, pool_size=POOL_SIZE, lstm_output_size=LSTM_OUTPUT_SIZE,
                 language_model=LANGUAGE_MODEL, learning_rate=LEARNING_RATE):
        from keras.layers import Dropout, Conv1D, MaxPooling1D, LSTM, Dense
        from keras.models import Sequential
        from keras.optimizers import Adam

        # If the sequence length is not specified, use the largest number of tokens in any of the text. This is an
        # example of how model hyper-parameters might depend on the training data.
        label_names = training[2]
        if sequence_length is None:
            sequence_length = mycroft.text.longest_text(training[0], language_model)

        # The code here is identical to the code in the Keras example with two exceptions:
        model = Sequential()

        # 1. The embedding layer is generated by an embedding object.
        embedder = mycroft.text.TextSequenceEmbedder(vocabulary_size, sequence_length, language_model)
        model.add(embedder.embedding_layer_factory()(input_length=sequence_length, trainable=False))

        model.add(Dropout(dropout))
        model.add(Conv1D(filters, kernel_size, padding="valid", activation="relu", strides=1))
        model.add(MaxPooling1D(pool_size=pool_size))
        model.add(LSTM(lstm_output_size))

        # 2. The final fully-connected layer has been modified to handle multiclass classification instead of just
        # binary.
        model.add(Dense(len(label_names), activation="softmax", name="softmax"))
        optimizer = Adam(lr=learning_rate)
        model.compile(optimizer=optimizer, loss="sparse_categorical_crossentropy", metrics=["accuracy"])

        # This passes a Keras model, a mycroft.text.Embedder, and the label names back to the parent constructor.
        super().__init__(model, embedder, label_names)

    # Derived classes may optionally supply a custom_command_line_options class method that returns a dictionary that
    # specifies additional keyword arguments to provide to the argparse.addArgument command. Here they are used to
    # provide additional formatting to the help text.
    @classmethod
    def custom_command_line_options(cls):
        return {
            "sequence_length": {"help": "Maximum number of tokens per text (default use longest in the data)",
                                "type": int,
                                "metavar": "LENGTH"},
            "vocabulary_size": {"help": "number of words in the vocabulary (default %d)" % cls.VOCABULARY_SIZE,
                                "metavar": "SIZE"},
            "dropout": {"help": "Dropout rate (default %0.2f)" % cls.DROPOUT},
            "filters": {"help": "Number of filters (default %d)" % cls.FILTERS},
            "kernel_size": {"help": "Size of kernel  (default %d)" % cls.KERNEL_SIZE, "metavar": "SIZE"},
            "pool_size": {"help": "Size of pooling layer (default %d)" % cls.POOL_SIZE, "metavar": "SIZE"},
            "lstm_output_size": {"help": "LSTM output size (default %d)" % cls.LSTM_OUTPUT_SIZE, "metavar": "SIZE"},
            "learning_rate": {"metavar": "RATE", "help": "learning rate (default %0.2f)" % cls.LEARNING_RATE},
            "language_model": {"help": "Language model (default %s)" % cls.LANGUAGE_MODEL, "metavar": "MODEL"}
        }


if __name__ == "__main__":
    # The main function incorporates the custom model into Mycroft's command line framework.
    mycroft.console.main([(ConvolutionNetClassifier, "convnet", "Train a convolution network classifier")],
                         description="Convolution Network")
